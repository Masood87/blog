---
title: Selection of Provinces for Monitoring in SAP 2018
author: ''
date: '2018-06-10'
slug: selection-of-provinces-for-monitoring-in-sap-2018
categories:
  - Afghanistan
tags:
  - AfghanSurvey
  - EDA
---

```{r include=FALSE, }
library(tidyverse)
```

# Introduction

The Asia Foundation (TAF) employs a number of methods to ensure data quality for the Survey of the Afghan People (SAP). The most important quality controls are (1) third party monitoringy, and (2) logic tests. The third party monitoring is carried out on 10% of all surveys by independent companies. The **third party monitoring** started in 2014, and since then Sayara Research has been the implementing partner for this activity. The **logic tests** are a set of internal tools in the form of codes that are used to detects inconsistencies and potentially fraudulent data. In this note, I document the analysis and reasoning behind the selection of 13 provinces for _third party monitoring_.

# Data

TAF aims to use monitoring to improve the quality of fieldwork and the collected data, and employs monitoring in the most problematic provinces. In 2017, 13 provinces were monitored, and TAF wants to keep the same number of provinces for 2018 as well. To select 13 most problematic provinces, a number of documents and resources are consulted to finalize the list of provinces. The following are a list of documents and resources consulted for this task:

1. Logic test results in 2017
2. SAP sample in 2017
3. Third Party Monitoring report in 2017
4. Advices from the fieldwork and monitoring companies (ACSOR and Sayara)

# Analysis

```{r echo=FALSE, message=FALSE, warning=FALSE}
lt <- read_csv("~/Documents/SAP 2018/Contracts/Sayara/Province selection based on SAP 2017/SAP 2017 LT report by province.csv")
varnames <- lt[2,] %>% as.character()
lt <- lt[3:nrow(lt),]
colnames(lt) <- varnames
lt <- lt[, c(1, 2, 5, 8, 10)]
str(lt)
lt <- mutate(lt, `proportion of LT failures` = round(as.numeric(`proportion of LT failures`)*100, 2), sayara = factor(case_when(sayara == "0" ~ "Not Monitored", sayara == "1" ~ "Monitored"), levels = c("Not Monitored", "Monitored"), ordered = TRUE))
# lt[[5]] <- lt[[5]] %>% as.numeric() %>% {round(.*100, 2)}
colnames(lt) <- c("Provinces", "Logic Test Failures", "Sample Size", "Province Monitored", "Percent Failed at Logic Test")
```

The logic test results are aggregated at the provincial level at this analysis. In addition to the failtures, we look at the provincial sample size and whether the province was monitored or not (by Sayara). The following table shows a snippet of the data used in this section of analysis. 

```{r}
knitr::kable(head(lt))
failure <- group_by(lt, `Province Monitored`) %>% summarise(mean_failure = mean(`Percent Failed at Logic Test`)) %>% spread(`Province Monitored`, mean_failure)
```

Looking at the logic test failure rates by province, and disaggregated by whether the provinces are monitored or not, it becomes evident that the monitored provinces have lower rates on average. It is notworthy that monitored provinces were chosen to be monitored because of their bad performance in the previous wave of the survey. Therefore, it is possible that the imrpovement in data quality as inferred from lower logic test failures are because of the monitoring of the fieldwork.

```{r echo=FALSE}
ggplot(lt, aes(x = Provinces, y = `Percent Failed at Logic Test`, fill = `Province Monitored`)) + geom_col() + facet_wrap(~`Province Monitored`, scales = "free_x") + theme_linedraw() + theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1), legend.position = "none") + labs(x = "") + geom_hline(yintercept = c(failure$`Not Monitored`, failure$Monitored))
```





